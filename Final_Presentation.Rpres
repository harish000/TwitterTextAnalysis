Twitter Text Analytics with R 
========================================================
author: Sriharish Ranganathan, Aravindharaj Rajendran, Vasudev Ramachandran, Praveen Kumar, Geethamsi Singavarapu
date: 
autosize: true
width: 1920
height: 1080

```{r setup, include=FALSE}
opts_chunk$set(cache=FALSE)
```
Introduction
========================================================



<p text-align: left;> R is open source, statistics, data management, and graphics platform maintained and developed by a community of developers. </p>

<p text-align: left;> <b>R</b> is a tool for </p>

* Data Manipulation
    + Connecting to data source
    + Cleaning and wrangling data
  
* Modelling and Computation
    + Statistical modelling
    + Numerical computation
  
* Data Visualization
    + Interactive fit and plots
    + Composing statistical graphics


Introduction 
========================================================

* Twitter is the most sought out social media platform and about 80% of the twitter user tends to update their day to day activity. Thus, it is easy to understand the user behavior pattern by mining these tweets. 

* Our project is aimed in understanding the current sentiment of the people towards the 2016 **Presidential Election** and the top two fore-running Candidates, based upon the public’s live opinions and emotions from twitter rather than smaller,localized polls. 
<center>![](twitter-analytics.png.png)</center>


* We also identify key topics to find relationship between the candidates and the tweets. 

* Various statistical packages in R is used for data collection, cleaning, wrangling, analysis and visualization

* The “Comprehensive R Archive Network" provides access to these dedicated packages for performing these operations


Data Collection
========================================================
* Twitter provides Streamming API with OAuth Connection to extract data based on user handlers or hashtags. 
```{r, eval=FALSE, include=FALSE}

requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
twitter_consumer_key <- "J9o6Ab4eDffpPWBYFpgB0CYzv" 
twitter_consumet_secret <- "JXX8Xa6H2vo0wZEuZSBugVqmv12eHc0rPvBp0cOAjC3taOcFY0"
twitter_access_token <- "301280862-Y3sXqyHO2nb6hgFLSlv8L6jejZb2AkYptT2WSRRU"
twitter_access_secret <- "Xc0xXYWA1XUPgcDA9f6VWDapT0OUu2UluAwPA2XslkpkQ"

my_oauth <- OAuthFactory$new(consumerKey=twitter_consumer_key,
                             consumerSecret=twitter_consumet_secret,
                             requestURL=requestURL,
                             accessURL=accessURL,
                             authURL=authURL)

my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
save(my_oauth, file="oauth_token.Rdata")
setup_twitter_oauth(consumer_key = twitter_consumer_key,
                    consumer_secret = twitter_consumet_secret,
                    access_token = twitter_access_token,
                    access_secret = twitter_access_secret)
filterStream(file.name="user_tweets1.json",
             track=c("election2016", "Election2016","Hillary", "hillary","trump",
                     "Trump","donald","Donald","Bernie","bernie","Ted","cruze","republican","democrats"),
             locations = c(-125, 25, -66, 50), timeout=1500, oauth=my_oauth)

```

* Keywords can also be given as a input parameter to extract data. 

```
filterStream(file.name="user_tweets.json",
             track=c("election2016", "Election2016","Hillary", "hillary","trump", "Trump","donald",
                     "Donald","Bernie","bernie","Ted","cruze", "republican","democrats"),
             locations = c(-125, 25, -66, 50), timeout=1500, oauth=my_oauth)
```
* The tweets are extracted in JSON file format which needs to be parsed into a R Data Frame. Data Frames provides table like data structure which will be easy for further manupulation and analysis. 

* Some of the attributes from the dataset are:

First Header  | Second Header                                                 |
------------- | --------------------------------------------------------------|
text          | The actual body of the tweet                                  |
id_str        | Provides a unique identifier for the tweet                    |
created_at    | Gives the timestamp of the tweet when it is created           |
location      | Gives the name of the location from which the tweet was posted|
lat, lon      | Denotes the coordinates of the tweet location.                |

Data Cleaning
========================================================

* Twitter text is unstructured. It contains **Slang, Emoticons and Abbtrivations** which needs to be filtered out. 

```
tweets_df_geo$text <- gsub('@[0-9_A-Za-z]+', '@', tweets_df_geo$text)
tweets_df_geo$text <- sapply(tweets_df_geo$text, function(row) iconv(row, "latin1", "ASCII", sub=""))
```

* After initial data cleaning we get the following basic plot
 
```{r, fig.align='center', warning=FALSE, include=FALSE}

tweets_df_geo <- read.csv("dataset_final.csv", header = TRUE)

#getting tweets that has location parameter
library(maps)
library(ggplot2)
map.data <- map_data("state")

ggplot(map.data) + geom_map(aes(map_id = region), map = map.data, fill = "white",
                            color = "grey50", size = 0.25) + expand_limits(x = map.data$long, y = map.data$lat) + 
  # 2) limits for x and y axis
  scale_x_continuous(limits=c(-125,-66)) + scale_y_continuous(limits=c(25,50)) +
  # 3) adding the dot for each tweet
  geom_point(data = tweets_df_geo, 
             aes(x = lon, y = lat), size = 1, alpha = 1/5, color = "black") +
    coord_fixed(ratio=1) +
  # 4) removing unnecessary graph elements
  theme(axis.line = element_blank(), 
        axis.text = element_blank(), 
        axis.ticks = element_blank(), 
        axis.title = element_blank(),
        panel.background = element_blank(), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        plot.background = element_blank())

```
<center><img src="Tweets_location.png" height=50% width=50% align:center></center>

* We also performed some natural language process such as **Tokenization and Stemming** to improve our sentiment accuracy.


Data Analysis - Relationships
========================================================
* Once we have the cleaned data, we performed some basic ananysis such as identifying relationship between the text. 
<center>![Relationship Between Terms](Relationship.png)</center>

```{r, include=FALSE}
library(quanteda)
library(topicmodels)
tweets_df_geo$text <- as.character(tweets_df_geo$text)
typeof(tweets_df_geo$text)
tweet_corpus <- corpus(tweets_df_geo$text)
tweets_dfm <- dfm(tweet_corpus, ignoredFeatures=c(stopwords("english"),"just", "t.co", "https", "rt", "amp", "http", "t.c", "can","@"))
tweets_dtm <- convert(tweets_dfm, to="topicmodels")
```
```{r, eval=FALSE, include=FALSE}
library(tm)
freq.terms <- findFreqTerms(tweets_dtm, lowfreq = 200)
```
```
library(tm)
freq.terms <- findFreqTerms(tweets_dtm, lowfreq = 200) 
freq.terms <-  freq.terms[!is.na(freq.terms)]
```


Data Analysis - Common Hashtags
========================================================
```{r}
source("functions.R")
hashtags <- getCommonHashtags(tolower(tweets_df_geo$text))
hashtags[1:20]
```


Data Analysis - Identifying Sentiments
========================================================

## Data Analysis - Identifying Sentiments

* We follow two approach to identify sentiments in the tweet text
    + Bag of Words Approach
    + Supervised Machine Learning 
    
* Bag of Words Approach
    + In this approach, we have a lexicon of words with their corresponding sentiments which are mapped with the tweet text to identify the sentiments
    + Quick and dirty method
* Supervised Machine Learning
    + Here we train a part of the data to identify sentiment and apply the the model on the test data
    + Regularized linear regression is used. 
    
```{r, include=FALSE}
library(Rstem)
source("functions.r")
#Sentiment Analysis using 'Bag of Words' approach

#Initial data cleaning
tweets_df_geo$text <- gsub('@[0-9_A-Za-z]+', '@', tweets_df_geo$text)
tweets_df_geo$text <- sapply(tweets_df_geo$text, function(row) iconv(row, "latin1", "ASCII", sub=""))

#getting the bag of words
lexicon <- read.csv("lexicon.csv", stringsAsFactors=F)

#Seperating the positive and negative words
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]

text <- clean_tweets(tweets_df_geo$text)
classifier(text, pos.words, neg.words)

```

```{r, echo=TRUE}
classifier(text, pos.words, neg.words)
```

```{r, include=FALSE}
library(quanteda)
#Getting corpus of input using corpus function in quantada package
tweet_corpus <- corpus(tweets_df_geo$text)
mydict <- dictionary(list(negative = neg.words,positive = pos.words))

#Getting the document frequency matrix
sent <- dfm(tweet_corpus, dictionary = mydict)

tweets_df_geo$score <- as.numeric(sent[,2]) - as.numeric(sent[,1])

#Meanest tweet
tweets_df_geo[which.min(tweets_df_geo$score),]

#Happiest tweet
tweets_df_geo[which.max(tweets_df_geo$score),]


tweets_df_geo$sentiment <- "neutral"
tweets_df_geo$sentiment[tweets_df_geo$score<0] <- "negative"
tweets_df_geo$sentiment[tweets_df_geo$score>0] <- "positive"

```
* This sentiment is added to the original dataframe which can be further visualized

Data Analysis - Intrepreting Sentiments
========================================================
```{r, include=FALSE}
library(stringr)

related_to <- data.frame(related_to=character(),stringsAsFactors = FALSE)
for(i in 1:nrow(tweets_df_geo)) {
  row <- tweets_df_geo[i, ]
  trump_count <- 0
  hillary_count  <- 0
  trump_count <- str_count(tolower(row$text), c("trump","donald"))
  hillary_count <- str_count(tolower(row$text), c("hillary","clinton"))
  if (trump_count == 0 && hillary_count == 0) {
    related_to[i, 1] <- "None"
  }  else if (trump_count > hillary_count) {
    related_to[i, 1] <- "Trump"
  } else if (hillary_count > trump_count) {
    related_to[i, 1] <- "Clinton"
  } else if (trump_count == hillary_count) {
    related_to[i, 1] <- "Neutral"
  }
}
tweets_df_geo$related_to <- related_to[,1]
tweets_df_geo <- tweets_df_geo[tweets_df_geo$related_to !="Neutral" & tweets_df_geo$related_to!="None", ]


```
<center>
```{r, echo=FALSE, fig.align='center'}
library(plotly)
library(ggplot2)
#Plotting Map using Plotly 
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showland = TRUE,
  landcolor = toRGB("gray95"),
  subunitcolor = toRGB("black"),
  countrycolor = toRGB("gray"),
  countrywidth = 1,
  subunitwidth = 1
  
)

#Removing tweets that are irrelevant
p <- plot_geo(tweets_df_geo, lat = tweets_df_geo$lat, lon = tweets_df_geo$lon) %>%
  add_markers(    text = ~paste(tweets_df_geo$related_to,tweets_df_geo$sentiment, tweets_df_geo$place_name, sep = "<br />"),
      color= tweets_df_geo$related_to,colors=c('blue','red'), symbol = I("circle"), size = I(8), hoverinfo = "text"
  ) %>%
  layout(
    title = 'Tweets by location<br />(Hover for more information)', geo = g
  )
     
htmlwidgets::saveWidget(as.widget(p), file = "demo.html")

```
</center>
<iframe src="demo.html" style="position:absolute;height:100%;width:100%"></iframe>


Data Analysis - Topic Modelling 
========================================================
* Topic models are a statistical model for Natural Language Processing that allows us to identify the key topics in the corpus 
* In general, text mining, topic modelling serves as a good platform to identify the key concepts discussed in the text. 

```
library(RColorBrewer)
plot(tweets_dfm, scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250)
```
<center><img src="wordCloud_no_tfidf.png" height=70% width=70% align:center></center>


Data Analysis - Applying LDA
========================================================

*  One of the most popular and accurate models for extracting key topics is Latent Dirichlet Allocation based on Bayesian Statistics
*  The model takes a Term-Document Matrix and a parameter k as input and generates k topic from each document. 
*  The main objective of topic modelling is to generate keywords or topics that are more relevant to the context of the tweet text. 
<center><img src="topics.png" height=70% width=70% align:center></center>

```{r, include=FALSE}
library(LDAvis)
library(jsonlite)
source("functions.r")
tweets_dtm <- convert(tweets_dfm, to="topicmodels")
lda <- LDA(tweets_dtm, k = 20, method = "Gibbs", 
           control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
term <- terms(lda, 10)
colnames(term) <- paste("Topic",1:20)
json <- topicmodels_json_ldavis(lda,tweets_dfm,tweets_dtm)
new.order <- fromJSON(json)$topic.order


serVis(json, out.dir = 'ElectionLDA', open.browser = TRUE)

```

Conclusion
========================================================


*  The Good
    +  Effectively the lingua franca of data analysis and statistical computing
    +  Free and open source
    +  As a statistical language, it’s generally considered to be very easy to code when compared to SAAS, SPSS
    +  Native cross-platform and 64-bit support
    +  Typically easy to install and configure
    +  Community of millions of users brilliant minds with rapidly growing number of packages 
    
*  The Bad
    + Poor parallelization
    + Requires more system resource for large dataset
    + Poor memory performance, difficulty handing big data
    + Can be difficult to compile base R and R packages from source
